{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZTzmh7TEB_U"
   },
   "source": [
    "### Uniquement en utilisant la comparaison lexicale des mots des deux documents et en utilisant WodNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YywwDOyq4yqd",
    "outputId": "03f146c0-ca22-467f-c50e-45899839b6af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "'''It provides a wide range of tools and resources for various natural language processing (NLP) tasks, such as tokenization,\n",
    "stemming, lemmatization, part-of-speech tagging, parsing, semantic reasoning,'''\n",
    "from nltk.corpus import wordnet\n",
    "'''WordNet is a lexical database and semantic network for the English language. It is a widely used resource in \n",
    "natural language processing (NLP) and computational linguistics. \n",
    "WordNet organizes words into synsets, which are sets of synonyms representing different senses of a word.'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "nltk.download('punkt') #The Punkt tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "#Averaged Perceptron Tagger is one of the taggers available in NLTK \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5Xp3kxCEIol",
    "outputId": "01c8ccd3-cfed-4ffc-83a5-8ae07fba2979"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_set1 inter word_set2{'.', 'mat'}\n",
      "word_set1 inter word_set2{'.', 'mat'}\n",
      "Similarité lexicale: 0.4\n",
      "Similarité WordNet: 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Télécharger les ressources nécessaires de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenisation des mots\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Suppression des mots vides (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # to reduce words to their base or dictionary form, which is called the lemma\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def lexical_similarity(doc1, doc2):\n",
    "    tokens1 = preprocess_text(doc1)\n",
    "    tokens2 = preprocess_text(doc2)\n",
    "    \n",
    "    # Construction des ensembles de mots uniques pour chaque document\n",
    "    word_set1 = set(tokens1)\n",
    "    word_set2 = set(tokens2)\n",
    "    \n",
    "    # Calcul de la similarité lexicale en utilisant l'intersection des ensembles des mots\n",
    "    print('word_set1 inter word_set2'+str(word_set1.intersection(word_set2)))\n",
    "    print('word_set1 inter word_set2'+str(word_set2.intersection(word_set1)))\n",
    "    similarity = len(word_set1.intersection(word_set2)) / float(len(word_set1.union(word_set2)))\n",
    "    return similarity\n",
    "\n",
    "def wordnet_similarity(doc1, doc2):\n",
    "    tokens1 = preprocess_text(doc1)\n",
    "    tokens2 = preprocess_text(doc2)\n",
    "    \n",
    "    # Calcul de la similarité en utilisant WordNet\n",
    "    synsets1 = set()\n",
    "    synsets2 = set()\n",
    "    \n",
    "    # Obtenir les synsets pour chaque mot dans le premier document\n",
    "    for token in tokens1:\n",
    "        #the function wordnet.synsets(token) is \n",
    "        #used to retrieve the synsets (sets of synonymous words) associated with a given token or word.\n",
    "        synsets = wordnet.synsets(token)\n",
    "        synsets1.update(synsets)\n",
    "    \n",
    "    # Obtenir les synsets pour chaque mot dans le deuxième document\n",
    "    for token in tokens2:\n",
    "        synsets = wordnet.synsets(token)\n",
    "        synsets2.update(synsets)\n",
    "    \n",
    "    # Calcul de la similarité en utilisant le coefficient de similarité de path\n",
    "    #Path similarity calculates the similarity between two synsets based on the length\n",
    "    #of the shortest path that connects them in the WordNet hierarchy\n",
    "    max_similarity = 0\n",
    "    list_similarity=[]\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.path_similarity(synset2)\n",
    "            if similarity is not None and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "        list_similarity.append(max_similarity)\n",
    "        \n",
    "    max_=0\n",
    "    for i in list_similarity:\n",
    "        max_+=i\n",
    "    return max_/len(list_similarity)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "document1 = \"The cat sits on the mat.\"\n",
    "document2 = \"The dog is on the mat.\"\n",
    "\n",
    "lex_similarity = lexical_similarity(document1, document2)\n",
    "wrndt_similarity = wordnet_similarity(document1, document2)\n",
    "\n",
    "print(\"Similarité lexicale:\", lex_similarity)\n",
    "print(\"Similarité WordNet:\", wrndt_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rz9KtLRPESI3",
    "outputId": "f9176f09-d9a1-490b-b5c4-bbd57a260e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_set1 inter word_set2{'.'}\n",
      "word_set1 inter word_set2{'.'}\n",
      "Similarité lexicale: 0.2\n",
      "Similarité WordNet: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "document3 = \"everything everywhere all at once .\"\n",
    "document4 = \"The dog is on the mat.\"\n",
    "\n",
    "lex_similarity = lexical_similarity(document3, document4)\n",
    "wrdnt_similarity = wordnet_similarity(document3, document4)\n",
    "\n",
    "print(\"Similarité lexicale:\", lex_similarity)\n",
    "print(\"Similarité WordNet:\", wrdnt_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbMzl8eW707a"
   },
   "source": [
    "<h4>Question2-2 <h4>:  En utilisant un corpus de votre choix (de taille convenable aux capacit´es\n",
    "  de votre machine g´en´erer des word embedding sp´ecifiques destin´ees au\n",
    "domaine de l’informatique avec deux m´ethodes diff´erentes:\n",
    "\n",
    "\n",
    "-    en utilisant la bibliotheque Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aitma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.similarities import WmdSimilarity\n",
    "import gensim.downloader as api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str)->list[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text into a list of words.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens representing the words in the input text.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str)-> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by converting it to lowercase, removing punctuation, and digits.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove all punctuation\n",
    "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    #remove any digits (numbers)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "text=preprocess(text)\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: . | Embedding: [-0.01787424  0.00788105  0.17011166]\n",
      "Word: to | Embedding: [ 0.30019328 -0.3115975  -0.23645042]\n",
      "Word: algorithms | Embedding: [ 0.2149941   0.2985288  -0.16695288]\n",
      "Word: , | Embedding: [-0.1254006   0.24475677 -0.05086717]\n",
      "Word: machine | Embedding: [-0.15153104  0.21755512 -0.16200581]\n",
      "Word: learning | Embedding: [-0.0605833   0.0947827   0.03317054]\n",
      "Word: is | Embedding: [-0.27607957 -0.31564498  0.24397875]\n",
      "Word: of | Embedding: [0.16900873 0.22525644 0.02542885]\n",
      "Word: a | Embedding: [ 0.21164015 -0.11510277 -0.03076098]\n",
      "Word: as | Embedding: [ 0.19207078 -0.25213814 -0.13087903]\n",
      "Word: in | Embedding: [-0.25053084 -0.03176463  0.31792405]\n",
      "Word: or | Embedding: [-0.24378148 -0.07828728 -0.06436103]\n",
      "Word: it | Embedding: [ 0.26936173 -0.19787991  0.00153247]\n",
      "Word: data | Embedding: [-0.15830922 -0.320434    0.16695641]\n",
      "Word: the | Embedding: [-0.29163316 -0.14690854 -0.001092  ]\n",
      "Word: computer | Embedding: [-0.00973364 -0.25581473  0.32075828]\n",
      "Word: experience | Embedding: [ 0.16606128  0.3075863  -0.2719406 ]\n",
      "Word: through | Embedding: [ 0.15003718 -0.13845532  0.02756949]\n",
      "Word: sample | Embedding: [ 0.28357568 -0.149659    0.15060958]\n",
      "Word: study | Embedding: [-0.226232   -0.11828295  0.3132836 ]\n",
      "Word: on | Embedding: [-0.05246631  0.01018109 -0.137812  ]\n",
      "Word: based | Embedding: [-0.25603876 -0.05021316  0.08229232]\n",
      "Word: model | Embedding: [-0.02941656  0.18437311 -0.09147723]\n",
      "Word: mathematical | Embedding: [0.0753355  0.1818598  0.27819845]\n",
      "Word: build | Embedding: [-0.04843577 -0.30723783  0.14590521]\n",
      "Word: intelligence | Embedding: [ 0.0190689   0.24809712 -0.02709424]\n",
      "Word: artificial | Embedding: [-0.08794713 -0.29176697 -0.0285519 ]\n",
      "Word: subset | Embedding: [0.09420098 0.18003084 0.23512901]\n",
      "Word: that | Embedding: [-0.18993385  0.06174279  0.2030422 ]\n",
      "Word: improve | Embedding: [-0.1595384  -0.1038942   0.22671807]\n",
      "Word: automatically | Embedding: [0.05470871 0.00607693 0.11581358]\n",
      "Word: seen | Embedding: [0.00725925 0.32062754 0.16868679]\n",
      "Word: tasks | Embedding: [-0.29731947 -0.23507953  0.03013862]\n",
      "Word: known | Embedding: [ 0.21337987 -0.28783286  0.1223179 ]\n",
      "Word: variety | Embedding: [0.17316045 0.1908678  0.24904053]\n",
      "Word: perform | Embedding: [-0.20559967  0.03627032  0.20181416]\n",
      "Word: conventional | Embedding: [-0.0946435  -0.2064087  -0.01352848]\n",
      "Word: develop | Embedding: [-0.27868855 -0.18693669  0.23698205]\n",
      "Word: infeasible | Embedding: [0.11176074 0.2400202  0.22705409]\n",
      "Word: difficult | Embedding: [ 0.25129834 -0.12743887 -0.01813961]\n",
      "Word: where | Embedding: [ 0.07844625 -0.1510725   0.27965865]\n",
      "Word: vision | Embedding: [-0.3284578   0.22470735  0.09752795]\n",
      "Word: and | Embedding: [-0.1644176   0.14640011 -0.05792513]\n",
      "Word: filtering | Embedding: [ 0.2237559   0.33096358 -0.14495318]\n",
      "Word: email | Embedding: [-0.02021823 -0.19037744  0.12836798]\n",
      "Word: such | Embedding: [0.09280931 0.22885641 0.2039094 ]\n",
      "Word: applications | Embedding: [0.31801045 0.3088069  0.26334733]\n",
      "Word: wide | Embedding: [-0.23318577 -0.30534774 -0.01184986]\n",
      "Word: needed | Embedding: [-0.10324831  0.2625437   0.19796894]\n",
      "Word: used | Embedding: [-0.05187304  0.05021291  0.05950174]\n",
      "Word: are | Embedding: [ 0.26024535 -0.31728953 -0.00696901]\n",
      "Word: so | Embedding: [ 0.11581383 -0.03186314  0.27939034]\n",
      "Word: do | Embedding: [ 0.3003813   0.21748433 -0.02374979]\n",
      "Word: programmed | Embedding: [ 0.25710735 -0.2851991   0.10687505]\n",
      "Word: explicitly | Embedding: [-0.15428154 -0.16988648  0.11970988]\n",
      "Word: being | Embedding: [ 0.17914896  0.25885403 -0.19227846]\n",
      "Word: without | Embedding: [ 0.2475866   0.2205116  -0.12352779]\n",
      "Word: decisions | Embedding: [-0.2908303   0.18064407  0.2169182 ]\n",
      "Word: predictions | Embedding: [-0.02595764 -0.22425847 -0.23634847]\n",
      "Word: make | Embedding: [-0.08345264  0.1712554  -0.12222783]\n",
      "Word: order | Embedding: [-0.31217897  0.12704346  0.16293183]\n",
      "Word: training | Embedding: [-0.21416098  0.03988682 -0.0692648 ]\n"
     ]
    }
   ],
   "source": [
    "# train the Model\n",
    "model = Word2Vec(tokenized_sentences,vector_size=3, window=5,min_count=1,sg=0)\n",
    "\n",
    "# Obtenir tous les mots du vocabulaire\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "\n",
    "# Afficher chaque mot avec son embedding\n",
    "for word in words:\n",
    "    embedding = model.wv.get_vector(word)\n",
    "    print(f'Word: {word} | Embedding: {embedding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iZTzmh7TEB_U",
    "CtUQjOrCjGqW"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
